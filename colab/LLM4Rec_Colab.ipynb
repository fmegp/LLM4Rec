{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM4Rec - Colab Notebook\n",
        "\n",
        "This notebook runs the full CLLM4Rec pipeline on Colab:\n",
        "1. Training (content pretrain + iterative mutual training)\n",
        "2. Finetuning (recommendation head)\n",
        "3. Evaluation (Recall@20/40, NDCG@100)\n",
        "\n",
        "**Requirements:**\n",
        "- GPU runtime (Runtime → Change runtime type → GPU)\n",
        "- Hugging Face account (for GPT-2 model)\n",
        "- Weights & Biases account (for experiment tracking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Colab widget manager enabled\n",
            "Cloning repository...\n",
            "Working directory: /content/LLM4Rec\n",
            "Installing dependencies...\n",
            "\n",
            "Testing tqdm progress bar...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a412b932f3dd40499f8b7a71cf2f8ccd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Test:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ tqdm working\n",
            "============================================================\n",
            "Runtime Report\n",
            "============================================================\n",
            "Python: 3.12.12\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Torch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Numpy: 2.0.2\n",
            "\n",
            "✓ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# ====== Cell 1: Setup ======\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Enable Colab widget manager for tqdm progress bars\n",
        "try:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "    IN_COLAB = True\n",
        "    print(\"✓ Colab widget manager enabled\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Colab\")\n",
        "\n",
        "# Clone the repo\n",
        "REPO_URL = \"https://github.com/fmegp/LLM4Rec.git\"\n",
        "REPO_DIR = \"/content/LLM4Rec\"\n",
        "\n",
        "if IN_COLAB and not Path(REPO_DIR).exists():\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, REPO_DIR], check=True)\n",
        "    os.chdir(REPO_DIR)\n",
        "elif IN_COLAB:\n",
        "    os.chdir(REPO_DIR)\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Install dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"requirements-colab.txt\"], check=True)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"ipywidgets>=8.1.0\"], check=True)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"], check=True)\n",
        "\n",
        "# Add src to path\n",
        "src_path = str(Path(REPO_DIR if IN_COLAB else \".\").resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "# Verify tqdm works\n",
        "print(\"\\nTesting tqdm progress bar...\")\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "for _ in tqdm(range(10), desc=\"Test\", leave=True):\n",
        "    time.sleep(0.1)\n",
        "print(\"✓ tqdm working\")\n",
        "\n",
        "# Verify llm4rec imports\n",
        "from llm4rec.runtime import print_runtime_report\n",
        "print_runtime_report()\n",
        "print(\"\\n✓ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OUTPUT_DIR: /content/outputs/run_20251219_165107\n",
            "RUN_ID: 20251219_165107\n",
            "\n",
            "Note: This is ephemeral storage. Outputs will be lost when the runtime resets.\n"
          ]
        }
      ],
      "source": [
        "# ====== Cell 2: Output Directory (Ephemeral) ======\n",
        "\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUTPUT_DIR = f\"/content/outputs/run_{RUN_ID}\"\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
        "print(f\"RUN_ID: {RUN_ID}\")\n",
        "print(\"\\nNote: This is ephemeral storage. Outputs will be lost when the runtime resets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Login to Hugging Face to download GPT-2 model.\n",
            "Get your token from: https://huggingface.co/settings/tokens\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "178bb4f3311c4bf4b8ce8169ef06a096",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====== Cell 3: Hugging Face Login ======\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(\"Login to Hugging Face to download GPT-2 model.\")\n",
        "print(\"Get your token from: https://huggingface.co/settings/tokens\\n\")\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Login to Weights & Biases for experiment tracking.\n",
            "Get your API key from: https://wandb.ai/authorize\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfmenol\u001b[0m (\u001b[33mfmenol-csynbiosys\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ====== Cell 4: Weights & Biases Login ======\n",
        "\n",
        "import wandb\n",
        "\n",
        "print(\"Login to Weights & Biases for experiment tracking.\")\n",
        "print(\"Get your API key from: https://wandb.ai/authorize\\n\")\n",
        "\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "Downloading Google Drive file id=1G4t64tzAlXN0gq_0TJ5Wik8dsERz8pMJ to /content/data/raw/cllm4rec_dataset ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1G4t64tzAlXN0gq_0TJ5Wik8dsERz8pMJ\n",
            "From (redirected): https://drive.google.com/uc?id=1G4t64tzAlXN0gq_0TJ5Wik8dsERz8pMJ&confirm=t&uuid=e7bb28fc-ab7e-44e8-99a3-778b468b27a4\n",
            "To: /content/data/raw/cllm4rec_dataset\n",
            "100%|██████████| 334M/334M [00:53<00:00, 6.24MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset...\n",
            "\n",
            "DATASET_DIR: /content/data/raw/extracted/data/beauty\n",
            "\n",
            "Dataset layout validated:\n",
            "  - meta.pkl: /content/data/raw/extracted/data/beauty/meta.pkl\n",
            "  - train_matrix.npz: /content/data/raw/extracted/data/beauty/train_matrix.npz\n",
            "  - review.pkl: /content/data/raw/extracted/data/beauty/user_item_texts/review.pkl\n",
            "\n",
            "Initializing W&B run...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/LLM4Rec/wandb/run-20251219_165354-5w0z9jig</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fmenol-csynbiosys/cllm4rec/runs/5w0z9jig' target=\"_blank\">beauty_lambda1.0_20251219_165107</a></strong> to <a href='https://wandb.ai/fmenol-csynbiosys/cllm4rec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fmenol-csynbiosys/cllm4rec' target=\"_blank\">https://wandb.ai/fmenol-csynbiosys/cllm4rec</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fmenol-csynbiosys/cllm4rec/runs/5w0z9jig' target=\"_blank\">https://wandb.ai/fmenol-csynbiosys/cllm4rec/runs/5w0z9jig</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B run: https://wandb.ai/fmenol-csynbiosys/cllm4rec/runs/5w0z9jig\n",
            "\n",
            "✓ Dataset ready!\n"
          ]
        }
      ],
      "source": [
        "# ====== Cell 5: Dataset Download + W&B Init ======\n",
        "\n",
        "from pathlib import Path\n",
        "from llm4rec.io import (\n",
        "    download_gdrive,\n",
        "    safe_extract_archive,\n",
        "    validate_dataset_layout,\n",
        "    build_dataset_manifest,\n",
        "    save_json,\n",
        ")\n",
        "from llm4rec.logging_wandb import WandbHandle\n",
        "\n",
        "# Dataset config\n",
        "DATASET_NAME = \"beauty\"  # Options: beauty, sports, toys\n",
        "LAMBDA_V = 1.0\n",
        "DATA_GDRIVE_URL = \"https://drive.google.com/file/d/1G4t64tzAlXN0gq_0TJ5Wik8dsERz8pMJ/view?usp=drive_link\"\n",
        "\n",
        "# Download and extract\n",
        "RAW_DIR = Path(\"/content/data/raw\")\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ARCHIVE_PATH = RAW_DIR / \"cllm4rec_dataset\"\n",
        "EXTRACT_DIR = RAW_DIR / \"extracted\"\n",
        "\n",
        "if not ARCHIVE_PATH.exists():\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_gdrive(DATA_GDRIVE_URL, ARCHIVE_PATH, quiet=False)\n",
        "else:\n",
        "    print(\"Dataset already downloaded\")\n",
        "\n",
        "if not EXTRACT_DIR.exists():\n",
        "    print(\"Extracting dataset...\")\n",
        "    EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    safe_extract_archive(ARCHIVE_PATH, EXTRACT_DIR)\n",
        "else:\n",
        "    print(\"Dataset already extracted\")\n",
        "\n",
        "# Find dataset directory\n",
        "candidates = [p.parent for p in EXTRACT_DIR.rglob(\"meta.pkl\")]\n",
        "match = next((c for c in candidates if c.name == DATASET_NAME), candidates[0] if len(candidates) == 1 else None)\n",
        "if match is None:\n",
        "    raise RuntimeError(f\"Dataset '{DATASET_NAME}' not found. Available: {[c.name for c in candidates]}\")\n",
        "\n",
        "DATASET_DIR = str(match)\n",
        "print(f\"\\nDATASET_DIR: {DATASET_DIR}\")\n",
        "\n",
        "# Validate\n",
        "layout = validate_dataset_layout(DATASET_DIR)\n",
        "print(f\"\\nDataset layout validated:\")\n",
        "print(f\"  - meta.pkl: {layout.meta_path}\")\n",
        "print(f\"  - train_matrix.npz: {layout.train_matrix_path}\")\n",
        "print(f\"  - review.pkl: {layout.review_path}\")\n",
        "\n",
        "# Initialize W&B run (wrap in WandbHandle for stage functions)\n",
        "print(\"\\nInitializing W&B run...\")\n",
        "_wandb_run = wandb.init(\n",
        "    project=\"cllm4rec\",\n",
        "    name=f\"{DATASET_NAME}_lambda{LAMBDA_V}_{RUN_ID}\",\n",
        "    config={\n",
        "        \"dataset_name\": DATASET_NAME,\n",
        "        \"lambda_V\": LAMBDA_V,\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"output_dir\": OUTPUT_DIR,\n",
        "    },\n",
        ")\n",
        "wandb_handle = WandbHandle(run=_wandb_run, enabled=True)\n",
        "print(f\"W&B run: {_wandb_run.url}\")\n",
        "\n",
        "# Save manifest\n",
        "manifest = build_dataset_manifest(DATASET_DIR, include_optional=True)\n",
        "save_json(manifest, Path(OUTPUT_DIR) / \"dataset_manifest.json\")\n",
        "print(f\"\\n✓ Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training stage...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c0fd30127a04c7c88eff3bfd89d6b6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00a2a682f5e64bb58d59cc1ae28951c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fea130bbcbe4a779299e0aebabcb689",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d13e9bd25a544c92a7544ba5a1857084",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a820482abb1e46feb2b87930f03496b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "896d51de05eb4cae8e6b3b5de2018da6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----Begin Content GPT Pretraining Loop-----\n",
            "============================================================\n",
            "Content GPT Pretraining - Epoch 1/10\n",
            "============================================================\n",
            "Starting epoch 1, 3531 batches total...\n",
            "  batch 1/3531 loss=3.5876 avg=3.5876\n",
            "  batch 50/3531 loss=3.4814 avg=3.8058\n",
            "  batch 100/3531 loss=3.8337 avg=3.8014\n",
            "  batch 150/3531 loss=3.3728 avg=3.8160\n",
            "  batch 200/3531 loss=3.8996 avg=3.8041\n",
            "  batch 250/3531 loss=3.4805 avg=3.8040\n",
            "  batch 300/3531 loss=3.6270 avg=3.7861\n",
            "  batch 350/3531 loss=4.6844 avg=3.8023\n",
            "  batch 400/3531 loss=4.0420 avg=3.7913\n",
            "  batch 450/3531 loss=3.4581 avg=3.7864\n",
            "  batch 500/3531 loss=4.1565 avg=3.7865\n",
            "  batch 550/3531 loss=4.0093 avg=3.7891\n",
            "  batch 600/3531 loss=3.7890 avg=3.7860\n",
            "  batch 650/3531 loss=3.8487 avg=3.7840\n",
            "  batch 700/3531 loss=3.6102 avg=3.7885\n",
            "  batch 750/3531 loss=3.5979 avg=3.7907\n",
            "  batch 800/3531 loss=4.4340 avg=3.7929\n",
            "  batch 850/3531 loss=3.8138 avg=3.7900\n",
            "  batch 900/3531 loss=4.1332 avg=3.7896\n",
            "  batch 950/3531 loss=4.2196 avg=3.7919\n",
            "  batch 1000/3531 loss=3.9695 avg=3.7927\n",
            "  batch 1050/3531 loss=3.7335 avg=3.7902\n",
            "  batch 1100/3531 loss=4.2288 avg=3.7914\n",
            "  batch 1150/3531 loss=4.5664 avg=3.7905\n",
            "  batch 1200/3531 loss=4.0474 avg=3.7924\n",
            "  batch 1250/3531 loss=3.8040 avg=3.7915\n",
            "  batch 1300/3531 loss=4.1919 avg=3.7893\n",
            "  batch 1350/3531 loss=3.9050 avg=3.7876\n",
            "  batch 1400/3531 loss=3.9987 avg=3.7893\n",
            "  batch 1450/3531 loss=3.9816 avg=3.7899\n",
            "  batch 1500/3531 loss=3.6051 avg=3.7885\n",
            "  batch 1550/3531 loss=3.4540 avg=3.7851\n",
            "  batch 1600/3531 loss=4.0171 avg=3.7853\n",
            "  batch 1650/3531 loss=3.6191 avg=3.7822\n",
            "  batch 1700/3531 loss=3.7899 avg=3.7804\n",
            "  batch 1750/3531 loss=3.7865 avg=3.7773\n",
            "  batch 1800/3531 loss=4.1842 avg=3.7752\n",
            "  batch 1850/3531 loss=3.6145 avg=3.7741\n",
            "  batch 1900/3531 loss=3.6268 avg=3.7717\n",
            "  batch 1950/3531 loss=3.7627 avg=3.7719\n",
            "  batch 2000/3531 loss=3.7441 avg=3.7699\n",
            "  batch 2050/3531 loss=3.5900 avg=3.7688\n",
            "  batch 2100/3531 loss=3.5192 avg=3.7675\n",
            "  batch 2150/3531 loss=3.8125 avg=3.7666\n",
            "  batch 2200/3531 loss=3.8881 avg=3.7653\n",
            "  batch 2250/3531 loss=3.3574 avg=3.7642\n",
            "  batch 2300/3531 loss=3.6796 avg=3.7635\n",
            "  batch 2350/3531 loss=3.4038 avg=3.7617\n",
            "  batch 2400/3531 loss=3.8246 avg=3.7595\n",
            "  batch 2450/3531 loss=3.4407 avg=3.7579\n",
            "  batch 2500/3531 loss=3.5462 avg=3.7573\n",
            "  batch 2550/3531 loss=4.3362 avg=3.7561\n"
          ]
        }
      ],
      "source": [
        "# ====== Cell 6: Training ======\n",
        "\n",
        "from pathlib import Path\n",
        "from llm4rec.stages.training_stage import run_training\n",
        "\n",
        "HF_MODEL_NAME = \"openai-community/gpt2\"\n",
        "HF_CACHE_DIR = \"/content/hf_cache\"\n",
        "\n",
        "training_out = Path(OUTPUT_DIR) / \"training\"\n",
        "content_user = training_out / \"content\" / f\"user_embeddings_{LAMBDA_V}.pt\"\n",
        "collab_user = training_out / \"collaborative\" / f\"user_embeddings_{LAMBDA_V}.pt\"\n",
        "\n",
        "if content_user.exists() and collab_user.exists():\n",
        "    print(f\"Training artifacts already exist at {training_out}\")\n",
        "    print(\"Skipping training stage.\")\n",
        "else:\n",
        "    print(\"Starting training stage...\\n\")\n",
        "    run_training(\n",
        "        dataset_dir=DATASET_DIR,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        lambda_V=LAMBDA_V,\n",
        "        hf_model_name=HF_MODEL_NAME,\n",
        "        hf_cache_dir=HF_CACHE_DIR,\n",
        "        hf_token=None,  # Uses cached HF login\n",
        "        mixed_precision=\"bf16\",\n",
        "        wandb_handle=wandb_handle,\n",
        "    )\n",
        "    print(\"\\n✓ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Cell 7: Finetuning ======\n",
        "\n",
        "from pathlib import Path\n",
        "from llm4rec.stages.finetune_stage import run_finetuning\n",
        "\n",
        "training_out = Path(OUTPUT_DIR) / \"training\"\n",
        "finetune_out = Path(OUTPUT_DIR) / \"finetuning\"\n",
        "rec_user = finetune_out / \"rec\" / f\"user_embeddings_{LAMBDA_V}.pt\"\n",
        "\n",
        "if rec_user.exists():\n",
        "    print(f\"Finetuning artifacts already exist at {finetune_out}\")\n",
        "    print(\"Skipping finetuning stage.\")\n",
        "else:\n",
        "    print(\"Starting finetuning stage...\\n\")\n",
        "    run_finetuning(\n",
        "        dataset_dir=DATASET_DIR,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        pretrained_dir=str(training_out),\n",
        "        lambda_V=LAMBDA_V,\n",
        "        hf_model_name=HF_MODEL_NAME,\n",
        "        hf_cache_dir=HF_CACHE_DIR,\n",
        "        hf_token=None,\n",
        "        mixed_precision=\"bf16\",\n",
        "        wandb_handle=wandb_handle,\n",
        "    )\n",
        "    print(\"\\n✓ Finetuning complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Cell 8: Evaluation ======\n",
        "\n",
        "from pathlib import Path\n",
        "from llm4rec.stages.eval_stage import run_eval\n",
        "\n",
        "finetune_out = Path(OUTPUT_DIR) / \"finetuning\"\n",
        "\n",
        "print(\"Starting evaluation...\\n\")\n",
        "results = run_eval(\n",
        "    dataset_dir=DATASET_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    rec_embeddings_dir=str(finetune_out / \"rec\"),\n",
        "    lambda_V=LAMBDA_V,\n",
        "    hf_model_name=HF_MODEL_NAME,\n",
        "    hf_cache_dir=HF_CACHE_DIR,\n",
        "    hf_token=None,\n",
        "    wandb_handle=wandb_handle,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*50)\n",
        "for k, v in results.items():\n",
        "    if isinstance(v, float):\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "# Finish W&B run\n",
        "wandb.finish()\n",
        "print(f\"\\n✓ Run complete! Results saved to {OUTPUT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
